---
title: "Project 2"
author: "Grace Holliday and Damon D'Ambrosio"
date: "2023-07-06"
output: github_document
params: 
  datachannel:
    label: 'Data Channel'
    value: channels[i]
    input: select
    choices: channels
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(gbm)
library(randomForest)
```

# Data Channel: **`r params[[1]][1]`**

# Introduction 

The goal/purpose of this project is to explore the factors influencing the 
amount of shares (popularity) of articles published by Mashable in a period of 
two years.

The data includes 39,644 observations of 61 total variables.  For our
exploration, we will use variables num_imgs, num_videos, num_keywords, 
weekday_is variables (including is_weekend), global_subjectivity (Text 
subjectivity),global_sentiment_polarity (Text sentiment polarity), 
global_rate_positive_words (Rate of positive words in the content), 
global_rate_negative_words (Rate of negative words in the content), 
title_subjectivity, and title_polarity, to explore the target variable (shares).

The methods we will use to model the response include:  
- Linear Regression  
- Random Forest  
- Boosted Tree  

# Data

```{r read_in, results = "hide"}
dat <- read_csv("OnlineNewsPopularity.csv")

# Subset data to variables we want
dat <- dat[,-c(1:9,12,20:31,39:44, 49:55,58,59)]

# Subsetting to correct channel(s)
col <- params[[1]][1]
if(col=="data_channel_is_lifestyle")
{
  dattype <- subset(dat,data_channel_is_lifestyle==1)
} else if(col=="data_channel_is_entertainment")
{
  dattype <- subset(dat,data_channel_is_entertainment==1)
} else if(col=="data_channel_is_bus")
{
  dattype <- subset(dat,data_channel_is_bus==1)
} else if(col=="data_channel_is_socmed")
{
  dattype <- subset(dat,data_channel_is_socmed==1)
} else if(col=="data_channel_is_tech")
{
  dattype <- subset(dat,data_channel_is_tech==1)
} else if(col=="data_channel_is_world")
{
  dattype <- subset(dat,data_channel_is_world==1)
}

# Removing data channel columns
dattype <- dattype[,-c(4:9)]
```

# Summarizations  

First, we will produce basic summary statistics and plots about the training 
data. To do this, we first need to divide the data into a training and test 
set.

```{r data_setup}
# First, establishing every categorical variable as a character
# so can change the factor
dattype$weekday_is_monday <- as.character(dattype$weekday_is_monday)
dattype$weekday_is_tuesday <- as.character(dattype$weekday_is_tuesday)
dattype$weekday_is_wednesday <- as.character(dattype$weekday_is_wednesday)
dattype$weekday_is_thursday <- as.character(dattype$weekday_is_thursday)
dattype$weekday_is_friday <- as.character(dattype$weekday_is_friday)
dattype$weekday_is_saturday <- as.character(dattype$weekday_is_saturday)
dattype$weekday_is_sunday <- as.character(dattype$weekday_is_sunday)

# Creating single variable for day 1 = Monday 2 = Tuesday 3 = Wednesday
# 4 = Thursday 5 = Friday 6 = Saturday 7 = Sunday 0 if no day, also
# making these a factor and not 0 and 1 for ensemble models

for(i in 1:nrow(dattype))
{
  if(dattype[i,4]==1)
  {
    dattype[i,19] <- 'Monday'
    dattype[i,4] <- 'Y'
    dattype[i,5] <- 'N'
    dattype[i,6] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'N'
  }
  else if (dattype[i,5]==1)
  {
    dattype[i,19] <- 'Tuesday'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'Y'
    dattype[i,6] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'N'
  }
  else if (dattype[i,6]==1)
  {
    dattype[i,19] <- 'Wednesday'
    dattype[i,6] <- 'Y'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'N'
  }
  else if (dattype[i,7]==1)
  {
    dattype[i,19] <- 'Thursday'
    dattype[i,6] <- 'N'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'N'
    dattype[i,7] <- 'Y'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'N'
    }
  else if (dattype[i,8]==1)
  {
    dattype[i,19] <- 'Friday'
    dattype[i,6] <- 'N'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'Y'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'N'
  }
  else if (dattype[i,9]==1)
  {
    dattype[i,19] <- 'Saturday'
    dattype[i,6] <- 'N'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'Y' 
    dattype[i,10] <- 'N'
  }
  else if (dattype[i,10]==1)
  {
    dattype[i,19] <- 'Sunday'
    dattype[i,6] <- 'N'
    dattype[i,4] <- 'N'
    dattype[i,5] <- 'N'
    dattype[i,7] <- 'N'
    dattype[i,8] <- 'N'
    dattype[i,9] <- 'N' 
    dattype[i,10] <- 'Y'
  }
  else
  {
    dattype[i,19] <- 'NA'
  }
}

colnames(dattype)[19] <- 'day'

# and back to a factor
dattype$weekday_is_monday <- as.factor(dattype$weekday_is_monday)
dattype$weekday_is_tuesday <- as.factor(dattype$weekday_is_tuesday)
dattype$weekday_is_wednesday <- as.factor(dattype$weekday_is_wednesday)
dattype$weekday_is_thursday <- as.factor(dattype$weekday_is_thursday)
dattype$weekday_is_friday <- as.factor(dattype$weekday_is_friday)
dattype$weekday_is_saturday <- as.factor(dattype$weekday_is_saturday)
dattype$weekday_is_sunday <- as.factor(dattype$weekday_is_sunday)
dattype$day <- as.factor(dattype$day)

# Setting order of new factor levels
dattype$day <- factor(dattype$day, levels =c('Monday', 'Tuesday', 'Wednesday',
                                  'Thursday', 'Friday', 'Saturday', 'Sunday'))

# First setting seed for reproducibility.
set.seed(1234)

# Divide data into training and test set.
train <- sample(1:nrow(dattype), size = nrow(dattype)*.70)
test <- setdiff(1:nrow(dattype), train)

# training and testing subsets
datTrain <- dattype[train, ]
datTest <- dattype[test, ]

```

## Summary Statistics

Now, we will conduct exploratory data analysis through creation of basic 
summary statistics and graphs exploring variables of interest to our 
response (shares) before we select variables for potential models.

### Mean Shares by Day

```{r summary1}
datTrain %>%
  group_by(day) %>%
  summarise_at(vars(shares), list(name = mean))
```

This summary reveals the average number of shares by day of the week
(Days 1-7, Mon-Sun).  The highest average represents the day with the most
shares.  A day of 0 means no day was specified.

### Total Multimedia (Images and Videos) by Day

```{r summary2}
datTrain %>% group_by(day) %>%
  mutate(total.media = num_imgs + num_videos) %>%
    group_by(day) %>%
      summarize(average = mean(total.media), median = median(total.media), 
                min = min(total.media), max = max(total.media),
                sd =   sd(total.media))
```

This summary provides the mean, median, minimum, maximum, and standard deviation
of the total multimedia objects (images + videos) appearing in articles by day. 
Potential trends in this summary (e.g., if articles published on certain day
have more multimedia objects), may indicate some type of collinearity between 
"day" and the "num_imgs" or "num_videos" variables.

## Graphs  

### Number of Images vs Shares

We can inspect the trend of shares as a function of the number of images.  If 
the points show an upward trend, then articles with more images tend to be 
shared more often.  If the points show a downward trend, then articles with 
less images tend to be shared more often.

```{r graph1}
plot(datTrain$shares~datTrain$num_imgs)
```

### Title Subjectivity vs Shares

We can inspect the trend of shares as a function of title subjectivity.  If the 
points show an upward trend, then articles with higher title subjectivity tend
to be shared more often.  If the points show a downward trend, then articles 
with lower title subjectivity tend to be shared more often.

```{r graph2}
plot(datTrain$shares~datTrain$title_subjectivity)
```

### Average Number of Keywords vs Shares

We can inspect the trend of shares as a function of number of keywords.  If the 
bars show an upward trend, then articles with more keywords tend to be shared
more often.  If the bars show a downward trend, then articles with less 
keywords tend to be shared more often.

```{r graph3}
datTrain$num_keywords <- as.factor(datTrain$num_keywords)
means <- datTrain %>%
  group_by(num_keywords) %>%
  summarise_at(vars(shares), list(name = mean))
barplot(height=means$name, names=means$num_keywords)
# back to numeric for later models
datTrain$num_keywords <- as.numeric(datTrain$num_keywords)
```

### Images vs Videos

We can inspect possible relationships between the number of images and videos 
in an article. If some type of relationship is evident (e.g., articles with
more images tend to have more videos), it may suggest some collinearity between
these two variables.

```{r graph4, message = FALSE}
g <- ggplot(datTrain, aes(x = scale(num_videos), y = scale(num_imgs)))

g + geom_point() + 
    xlab("Number of videos (standardized)") + 
    ylab("Number of images (standardized)") + 
    geom_smooth() + 
    theme_minimal()
```

### Positive vs negative word rates

We can inspect the relationship between positive and negative word rates in 
articles. If some type of relationship is evident (e.g., articles with higher 
positive word rates tend to have lower negative word rates), it may suggest 
some collinearity between these two variables.

```{r graph5, message = FALSE}
g <- ggplot(datTrain, aes(x = scale(global_rate_positive_words), y = scale(global_rate_negative_words)))

g + geom_point() + 
    xlab("Positive word rate (standardized)") + 
    ylab("Negative word rate (standardized)") + 
    geom_smooth() + 
    theme_minimal()
```

### Title subjectivity by day of week

We can inspect the relationship between title subjectivity and the day of the 
week to see if certain days tend to have more/less subjective article titles.

```{r graph6, message = FALSE}
g <- ggplot(datTrain, aes(x = day, y = title_subjectivity))

g + geom_bar(stat = "summary", fun = "mean", color = "black", fill = "blue") + 
    xlab("Day") + ylab("Title subjectivity") +
    theme_minimal()
```

# Modeling

Linear regression is an analysis method wherein the values of a variable 
(i.e., response variable) is predicted based upon the value of one more more 
other variables (i.e., predictor variables). There is an underlying assumption 
that there is a linear relationship between the response variable and any given 
predictor. The slope of this line is determined mathematically by minimizing 
the sum of the squared residuals. 

## Linear Models

This first model will explore the impact of images as combined with number of 
key words, title subjectivity, sentiment polarity, and positive word rate.

```{r lm_1}
#Train model
LinearModel1 <- train(shares ~ num_imgs+num_keywords+title_subjectivity+
                global_subjectivity+global_sentiment_polarity +
                global_rate_positive_words,
              data = datTrain,
              method = "lm",
              preProcess = c("center", "scale"), 
              trControl = trainControl(method = "cv", number = 5))

summary(LinearModel1)

#Run on test data
LinearModel1.predict <- predict(LinearModel1, newdata = datTest)

#Obtain RMSE from test set, which will be used in automated comparison
LinearModel1.compare <- postResample(LinearModel1.predict, obs = datTest$shares)
```

The second model explores the impact of videos instead of images. Keywords are 
retained as a variable. Instead of exploring subjectivity and postive word 
rates, this model explores the "negative" (i.e., global rate of negative words
and maximum polarity of negative words). It also includes an interaction 
between the number of videos and number of keywords.

```{r lm_2}
#Train model
LinearModel2 <- train(shares ~ num_videos + num_keywords + 
                 num_videos:num_keywords + global_rate_negative_words +                                     max_negative_polarity,
              data = datTrain,
              method = "lm",
              preProcess = c("center", "scale"), 
              trControl = trainControl(method = "cv", number = 5))

summary(LinearModel2)

#Run on test data
LinearModel2.predict <- predict(LinearModel2, newdata = datTest)

#Obtain RMSE from test set, which will be used in automated comparison
LinearModel2.compare <- postResample(LinearModel2.predict, obs = datTest$shares)

```


## Ensemble Models

To streamline model fitting, we will first remove the previously created "day" 
variable.

```{r remove_days}
# this is regression, not classification
#removing previously created day variable for boosted tree model
datTrain <- datTrain[,-19]
datTest <- datTest[,-19]
```

Now to fit the ensemble models.

### Random Forest Model
Random forests are similar to bagging in the sense that multiple trees 
(i.e., a "forest") are created from bootstrap samples of data, with the outcome 
of these trees averaged to form a conclusion. Unlike bagging, not all possible
predictors are used in each tree when using a random forest- only a random 
subset is used for each tree. This in turn makes the trees less correlated with 
one another, which allows for a greater reduction in variation once the 
outcomes of all trees are averaged.

Below is a random forest model to predict shares:

```{r random_forest}

#Create model. Constraining mtry to a max possible value of 12, which is ~1/3 of all possible variables
random.forest.fit <- train(shares ~ ., data = datTrain,
                           method = "rf",
                           preProcess = c("center", "scale"),
                           trControl = trainControl(method = "cv", number = 5),
                           tuneGrid = data.frame(mtry = 1:12))

# plot fit
plot(random.forest.fit)
random.forest.fit$finalModel

# test fit
test_x = datTest[, -18] 
test_y = datTest[, 18] 
pred_y = predict(random.forest.fit, test_x)
x_ax = 1:length(pred_y)
x_ax <- as.numeric(x_ax)
test_y <- test_y$shares 
test_y <- as.integer(test_y)
pred_y <- as.integer(pred_y)
plot(x_ax, test_y, col="blue", pch=20, cex=.9)
lines(x_ax, pred_y, col="red", pch=20, cex=.9)

#Run on test data
random.forest.predict <- predict(random.forest.fit, newdata = datTest)

#Obtain RMSE from test set, which will be used in automated comparison
random.forest.compare <- postResample(random.forest.predict, 
                                      obs = datTest$shares)
```

Here, the red lines are the predicted values for each observation.  If the red
lines are close to following the blue dots (which are representative of the
actual value for each observation), then the model is closely predicting.

### Boosted Tree Model  

Boosting is a general approach that can be applied to trees and allows for the 
slow training of trees.  The trees are grown sequentially with each subsequent 
tree being grown on a modified version of the original data.  The predictions
are then updated as the trees are grown, and new trees are grown by considering
the errors in the trees previously created. Lambda represents a shrinkage
parameter than slows the fitting process.

Below is a boosted tree model to predict shares.
```{r boosted_tree, results="hide"}
boostFit <- train(shares~., data=datTrain, method="gbm",
                      preProcess=c("center","scale"),
                      trControl=trainControl(method='cv',number=5),
                      tuneGrid = expand.grid(n.trees=seq(25,200,50),
                                             interaction.depth=seq(1,4,1),
                                             shrinkage=0.1,
                                             n.minobsinnode=10))
```

```{r boostfit plots}
# plot fit
plot(boostFit)
boostFit$finalModel

# test fit
test_x = datTest[, -18] 
test_y = datTest[, 18] 
pred_y = predict(boostFit, test_x)
x_ax = 1:length(pred_y)
x_ax <- as.numeric(x_ax)
test_y <- test_y$shares 
test_y <- as.integer(test_y)
pred_y <- as.integer(pred_y)
plot(x_ax, test_y, col="blue", pch=20, cex=.9)
lines(x_ax, pred_y, col="red", pch=20, cex=.9)

#Run on test data
boostFit.predict <- predict(boostFit, newdata = datTest)

#Obtain RMSE from test set, which will be used in automated comparison
boostFit.compare <- postResample(boostFit.predict, obs = datTest$shares)
```

Here, the red lines are the predicted values for each observation.  If the red
lines are close to following the blue dots (which are representative of the
actual value for each observation), then the model is closely predicting.


## Comparison

Now, grab all the RMSEs obtained from fitting the 4 models on the test set to
determine the "winner."

```{r comparison}
#Compile RMSEs from the 4 models
LinearModel1.RMSE <- LinearModel1.compare[1]
LinearModel2.RMSE <- LinearModel2.compare[1]
random.forest.RMSE <- random.forest.compare[1]
boostFit.RMSE <- boostFit.compare[1]

compare.all <- data.frame(
                    model = c("First Linear", "Second Linear", "Random Forest", 
                              "Boosted Tree"),
                    RMSE = c(LinearModel1.RMSE, LinearModel2.RMSE, 
                             random.forest.RMSE, boostFit.RMSE))

winner <- compare.all %>% slice_min(order_by = RMSE)
```

The lowest RMSE obtained on the test set was **`r round(winner[1,2],2)`**, which belongs 
to the **`r winner[1,1]`** model. Therefore, **`r winner[1,1]`** is the winner!

The final standings of all 4 models can be seen below:

```{r final_standings}
compare.all %>% arrange(RMSE) %>% mutate(Final.Rank = rank(RMSE))
```

